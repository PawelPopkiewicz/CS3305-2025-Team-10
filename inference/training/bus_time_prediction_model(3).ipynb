{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Bus Time Prediction Model\n",
        "# This notebook implements an encoder-decoder architecture to predict bus arrival times\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "AwR0K64m5S1n"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "TvUdv6W-5W3t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and preprocess the data\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load the bus stop data from a CSV file\"\"\"\n",
        "    df = pd.read_csv(file_path)  # Assuming space-separated values based on your example\n",
        "    print(f\"Loaded data with {len(df)} rows and {df.columns.tolist()} columns\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "9KVn2irC5IK2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_path = next(iter(uploaded))\n",
        "print(file_path)\n",
        "df = load_data(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "39KX7Mhr5ht_",
        "outputId": "cccffbef-0bff-4e15-a5f8-6f36caec7660"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-972f4a15-90dc-42c6-9a35-6819b76f6f63\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-972f4a15-90dc-42c6-9a35-6819b76f6f63\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test.csv\n",
            "test.csv\n",
            "Loaded data with 87545 rows and ['id', 'route_name', 'day', 'time', 'stop_id', 'scheduled_arrival_time', 'scheduled_departure_time', 'distance_to_stop', 'time_to_stop', 'residual_stop_time'] columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fields\n",
        "- id [string]\n",
        "- route_name [string, categorical]\n",
        "- day [int, categorical]\n",
        "- time [minutes, linear]\n",
        "- stop_distance [meters, linear]\n",
        "- scheduled_stop_time [seconds, linear]\n",
        "- residual_stop_time [seconds, linear]"
      ],
      "metadata": {
        "id": "xcc3ONdxhxNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Group data by trip_id\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess the data to prepare for model training\"\"\"\n",
        "    # Extract unique trip IDs\n",
        "    trip_ids = df['id'].unique()\n",
        "    print(f\"Found {len(trip_ids)} unique trips\")\n",
        "\n",
        "    # Extract unique route names for one-hot encoding\n",
        "    route_names = df['route_name'].unique()\n",
        "    print(f\"Found {len(route_names)} unique routes: {route_names}\")\n",
        "\n",
        "    # Create route name encoder\n",
        "    route_encoder = OneHotEncoder(sparse_output=False)\n",
        "    route_encoder.fit(df[['route_name']])\n",
        "\n",
        "    # Extract unique days for one hot encoding\n",
        "    days = df['day'].unique()\n",
        "    print(f\"Found {len(days)} unique days: {days}\")\n",
        "\n",
        "    # Create day encoder\n",
        "    day_encoder = OneHotEncoder(sparse_output=False)\n",
        "    day_encoder.fit(df[['day']])\n",
        "\n",
        "    # Create a scaler for time\n",
        "    time_scaler = StandardScaler()\n",
        "    time_scaler.fit(df[['time']])\n",
        "\n",
        "    # Create a scaler for stop distances\n",
        "    distance_scaler = StandardScaler()\n",
        "    distance_scaler.fit(df[['distance_to_stop']])\n",
        "\n",
        "    # Create a scaler for scheduled stop times\n",
        "    scheduled_time_scaler = StandardScaler()\n",
        "    scheduled_time_scaler.fit(df[['time_to_stop']])\n",
        "\n",
        "    # Create a scaler for residual stop times\n",
        "    residual_time_scaler = StandardScaler()\n",
        "    residual_time_scaler.fit(df[['residual_stop_time']])\n",
        "\n",
        "    return trip_ids, route_encoder, day_encoder, time_scaler, distance_scaler, scheduled_time_scaler, residual_time_scaler"
      ],
      "metadata": {
        "id": "7U3wrywU568D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trip_ids, route_encoder, day_encoder, time_scaler, distance_scaler, scheduled_time_scaler, residual_time_scaler = preprocess_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWqp4ZDq570J",
        "outputId": "01562876-83c4-47bf-a0bd-ed6931fb07a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4187 unique trips\n",
            "Found 23 unique routes: ['206' '203' '220' '202A' '202' '207' '216' '215' '214' '208' '207A' '205'\n",
            " '220X' '223' '219' '201' '215A' '225L' '225' '209A' '209' '223X' '212']\n",
            "Found 7 unique days: [6 0 1 2 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a custom dataset for the bus trip data\n",
        "class BusTripDataset(Dataset):\n",
        "    def __init__(self, df,\n",
        "                 trip_ids, route_encoder, day_encoder, time_scaler,\n",
        "                 distance_scaler, scheduled_time_scaler, residual_time_scaler,\n",
        "                 observed_ratio_range=(0.3, 0.9), max_target=15, max_stops=50, overlap=2):\n",
        "        \"\"\"\n",
        "        Dataset for bus trip prediction\n",
        "        \"\"\"\n",
        "        # Dataframe\n",
        "        self.df = df\n",
        "        # General Features\n",
        "        self.trip_ids = trip_ids\n",
        "        self.route_encoder = route_encoder\n",
        "        self.day_encoder = day_encoder\n",
        "        # Stop specific features\n",
        "        self.time_scaler = time_scaler\n",
        "        self.distance_scaler = distance_scaler\n",
        "        self.scheduled_time_scaler = scheduled_time_scaler\n",
        "        self.residual_time_scaler = residual_time_scaler\n",
        "        # Config\n",
        "        self.observed_ratio_range = observed_ratio_range\n",
        "        self.max_stops = max_stops\n",
        "        self.max_target = max_target\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trip_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        trip_id = self.trip_ids[idx]\n",
        "        trip_df = self.df[self.df['id'] == trip_id] #.sort_values('stop_distance')\n",
        "\n",
        "        # Ensure we don't exceed max_stops\n",
        "        if len(trip_df) > self.max_stops:\n",
        "            trip_df = trip_df.head(self.max_stops)\n",
        "\n",
        "        # Number of stops to use as observed\n",
        "        num_target = int(min(self.max_target, len(trip_df)//2))\n",
        "        num_observed = int(len(trip_df) - num_target)\n",
        "        # observed_ratio = random.uniform(*self.observed_ratio_range)\n",
        "        # num_observed = max(1, int(len(trip_df) * observed_ratio))\n",
        "        # num_observed = max(1, int(len(trip_df) * self.observed_ratio))\n",
        "\n",
        "        # Features for the trip\n",
        "        route_name = trip_df['route_name'].iloc[0]\n",
        "        day = trip_df['day'].iloc[0]\n",
        "\n",
        "        # One-hot encode the route name\n",
        "        route_name_df = pd.DataFrame([[route_name]], columns=[\"route_name\"])\n",
        "        route_encoded = route_encoder.transform(route_name_df)[0]\n",
        "\n",
        "        # One-hot encode the day\n",
        "        day_df = pd.DataFrame([[day]], columns=[\"day\"])\n",
        "        day_encoded = day_encoder.transform(day_df)[0]\n",
        "\n",
        "        # Create general trip features\n",
        "        trip_features = np.concatenate([\n",
        "            route_encoded,\n",
        "            day_encoded\n",
        "        ])\n",
        "\n",
        "        # Create arrays for all stops\n",
        "        stop_ids = trip_df['stop_id'].values\n",
        "        times = self.time_scaler.transform(trip_df[['time']]).flatten()\n",
        "        distances = self.distance_scaler.transform(trip_df[['distance_to_stop']]).flatten()\n",
        "        scheduled_times = self.scheduled_time_scaler.transform(trip_df[['time_to_stop']]).flatten()\n",
        "        residual_times = self.residual_time_scaler.transform(trip_df[['residual_stop_time']]).flatten()\n",
        "\n",
        "        # Split into observed and target\n",
        "        observed_stop_ids = stop_ids[:num_observed]\n",
        "        observed_times = times[:num_observed]\n",
        "        observed_distances = distances[:num_observed]\n",
        "        observed_scheduled_times = scheduled_times[:num_observed]\n",
        "        observed_residual_times = residual_times[:num_observed]\n",
        "\n",
        "        target_stop_ids = stop_ids[num_observed - self.overlap:]\n",
        "        target_times = times[num_observed - self.overlap:]\n",
        "        target_distances = distances[num_observed - self.overlap:]\n",
        "        target_scheduled_times = scheduled_times[num_observed - self.overlap:]\n",
        "        target_residual_times = residual_times[num_observed - self.overlap:]\n",
        "\n",
        "        # Pad sequences if needed\n",
        "        # max_observed = math.ceil(self.max_stops * self.observed_ratio_range[1])\n",
        "        # max_target = self.max_stops - math.floor(self.max_stops * self.observed_ratio_range[0])\n",
        "        max_observed = self.max_stops\n",
        "        max_target = self.max_target + self.overlap\n",
        "\n",
        "        # Pad observed sequences\n",
        "        if len(observed_stop_ids) < max_observed:\n",
        "            pad_length = max_observed - len(observed_stop_ids)\n",
        "            observed_stop_ids = np.pad(observed_stop_ids, (0, pad_length), 'constant', constant_values='PAD')\n",
        "\n",
        "            observed_times = np.pad(observed_times, (0, pad_length), 'constant', constant_values=0)\n",
        "            observed_distances = np.pad(observed_distances, (0, pad_length), 'constant', constant_values=0)\n",
        "            observed_scheduled_times = np.pad(observed_scheduled_times, (0, pad_length), 'constant', constant_values=0)\n",
        "            observed_residual_times = np.pad(observed_residual_times, (0, pad_length), 'constant', constant_values=0)\n",
        "\n",
        "        # Pad target sequences\n",
        "        if len(target_stop_ids) < max_target:\n",
        "            pad_length = max_target - len(target_stop_ids)\n",
        "            target_stop_ids = np.pad(target_stop_ids, (0, pad_length), 'constant', constant_values='PAD')\n",
        "\n",
        "            target_times = np.pad(target_times, (0, pad_length), 'constant', constant_values=0)\n",
        "            target_distances = np.pad(target_distances, (0, pad_length), 'constant', constant_values=0)\n",
        "            target_scheduled_times = np.pad(target_scheduled_times, (0, pad_length), 'constant', constant_values=0)\n",
        "            target_residual_times = np.pad(target_residual_times, (0, pad_length), 'constant', constant_values=0)\n",
        "\n",
        "        # Create mask for valid targets (non-padded values)\n",
        "        target_mask = (target_stop_ids != 'PAD').astype(float)\n",
        "\n",
        "        # Convert to tensors\n",
        "        trip_features = torch.tensor(trip_features, dtype=torch.float32)\n",
        "\n",
        "        observed_times = torch.tensor(observed_times, dtype=torch.float32)\n",
        "        observed_distances = torch.tensor(observed_distances, dtype=torch.float32)\n",
        "        observed_scheduled_times = torch.tensor(observed_scheduled_times, dtype=torch.float32)\n",
        "        observed_residual_times = torch.tensor(observed_residual_times, dtype=torch.float32)\n",
        "\n",
        "        target_times = torch.tensor(target_times, dtype=torch.float32)\n",
        "        target_distances = torch.tensor(target_distances, dtype=torch.float32)\n",
        "        target_scheduled_times = torch.tensor(target_scheduled_times, dtype=torch.float32)\n",
        "        target_residual_times = torch.tensor(target_residual_times, dtype=torch.float32)\n",
        "\n",
        "        target_mask = torch.tensor(target_mask, dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            'trip_features': trip_features,\n",
        "\n",
        "            'observed_times': observed_times,\n",
        "            'observed_distances': observed_distances,\n",
        "            'observed_scheduled_times': observed_scheduled_times,\n",
        "            'observed_residual_times': observed_residual_times,\n",
        "\n",
        "            'target_times': target_times,\n",
        "            'target_distances': target_distances,\n",
        "            'target_scheduled_times': target_scheduled_times,\n",
        "            'target_residual_times': target_residual_times,\n",
        "\n",
        "            'target_mask': target_mask,\n",
        "            'num_observed': num_observed,\n",
        "            'num_target': len(trip_df) - num_observed\n",
        "        }"
      ],
      "metadata": {
        "id": "XHdu3VyC6Eu-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating datasets...\")\n",
        "dataset = BusTripDataset(df,\n",
        "                 trip_ids, route_encoder, day_encoder, time_scaler,\n",
        "                 distance_scaler, scheduled_time_scaler, residual_time_scaler)\n",
        "print(f\"trips in dataset: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzcI_Spb8kwh",
        "outputId": "7f64ea23-c83e-48ce-a960-2a775dbc3097"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets...\n",
            "trips in dataset: 4187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_trip = dataset[0]\n",
        "print(example_trip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEczPhU19rfk",
        "outputId": "723cfccb-b8e3-407c-aa98-a727e18350fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'trip_features': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), 'observed_times': tensor([1.2548, 1.2564, 1.2580, 1.2597, 1.2613, 1.2629, 1.2661, 1.2693, 1.2710,\n",
            "        1.2726, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'observed_distances': tensor([-0.9600, -0.0217, -0.3413, -0.4399, -0.1216, -0.2694,  0.0071, -0.4887,\n",
            "        -0.1056, -0.4989,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000]), 'observed_scheduled_times': tensor([-1.1756, -0.6016, -0.6016, -0.6016, -0.6016, -0.6016, -0.0276, -0.0276,\n",
            "        -0.6016, -0.6016,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000]), 'observed_residual_times': tensor([-0.5718, -0.5629, -0.5859, -0.6166, -0.6101, -0.5491, -0.4568, -0.5420,\n",
            "        -0.5075, -0.5296,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000]), 'target_times': tensor([1.2710, 1.2726, 1.2742, 1.2758, 1.2774, 1.2790, 1.2839, 1.2887, 1.2919,\n",
            "        1.2952, 1.2984, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'target_distances': tensor([-0.1056, -0.4989, -0.0009, -0.5834, -0.4201,  0.0088,  0.4106,  0.1616,\n",
            "        -0.3277, -0.4607, -0.2312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000]), 'target_scheduled_times': tensor([-0.6016, -0.6016, -0.6016, -0.6016, -0.6016, -0.6016,  0.5465,  0.5465,\n",
            "        -0.0276, -0.0276, -0.0276,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000]), 'target_residual_times': tensor([-0.5075, -0.5296, -0.4804, -0.5149, -0.5258, -0.4904, -0.5697, -0.6222,\n",
            "        -0.6779, -0.7635, -0.8112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000]), 'target_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.]), 'num_observed': 10, 'num_target': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define the encoder-decoder model\n",
        "class BusTimeEncoder(nn.Module):\n",
        "    def __init__(self, trip_feature_dim, hidden_dim):\n",
        "        super(BusTimeEncoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Process trip features\n",
        "        self.trip_fc = nn.Sequential(\n",
        "            nn.Linear(trip_feature_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Process observed stops\n",
        "        self.stop_embedding = nn.Linear(4, hidden_dim)  # time, distance, scheduled, residual\n",
        "\n",
        "        # LSTM to process the sequence of stops\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, trip_features, observed_times, observed_distances, observed_scheduled_times, observed_residual_times):\n",
        "        # Process trip features\n",
        "        trip_embedding = self.trip_fc(trip_features)\n",
        "\n",
        "\n",
        "\n",
        "        # Combine distance and time for each stop\n",
        "        # batch_size = observed_distances.size(0)\n",
        "        # seq_length = observed_distances.size(1)\n",
        "        observed_features = torch.stack([observed_times, observed_distances, observed_scheduled_times, observed_residual_times], dim=2)\n",
        "\n",
        "        # Process each stop\n",
        "        observed_embedded = self.stop_embedding(observed_features)\n",
        "\n",
        "        # Initialize hidden state with trip features\n",
        "        h0 = trip_embedding.unsqueeze(0)\n",
        "        c0 = torch.zeros_like(h0)\n",
        "\n",
        "        # Process the sequence\n",
        "        output, (hn, cn) = self.lstm(observed_embedded, (h0, c0))\n",
        "\n",
        "        return output, (hn, cn)"
      ],
      "metadata": {
        "id": "aP79CZim6GPo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BusTimeDecoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim=1):\n",
        "        super(BusTimeDecoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Embedding for target residuals\n",
        "        self.stop_embedding = nn.Linear(3, hidden_dim)\n",
        "\n",
        "        # LSTM for decoding\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, target_times, target_distances, target_scheduled_times, encoder_hidden):\n",
        "\n",
        "        target_features = torch.stack([target_times, target_distances, target_scheduled_times], dim=2)\n",
        "        # Embed the target distances\n",
        "        #target_residuals = target_residuals.unsqueeze(2) # Add feature dimension\n",
        "\n",
        "        target_embedded = self.stop_embedding(target_features)\n",
        "\n",
        "        # Use encoder's hidden state\n",
        "        output, _ = self.lstm(target_embedded, encoder_hidden)\n",
        "\n",
        "        # Generate time predictions\n",
        "        residual_pred = self.fc_out(output)\n",
        "\n",
        "        return residual_pred.squeeze(2)"
      ],
      "metadata": {
        "id": "yvLxAidI6KkY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BusTimeEncoderDecoder(nn.Module):\n",
        "    def __init__(self, trip_feature_dim, hidden_dim):\n",
        "        super(BusTimeEncoderDecoder, self).__init__()\n",
        "        self.encoder = BusTimeEncoder(trip_feature_dim, hidden_dim)\n",
        "        self.decoder = BusTimeDecoder(hidden_dim)\n",
        "\n",
        "    def forward(self, trip_features, observed_times, observed_distances, observed_scheduled_times, observed_residual_times, target_times, target_distances, target_scheduled_times):\n",
        "        # Encode the observed stops\n",
        "        _, encoder_hidden = self.encoder(trip_features, observed_times, observed_distances, observed_scheduled_times, observed_residual_times)\n",
        "\n",
        "        # Decode to predict target times\n",
        "        residual_pred = self.decoder(target_times, target_distances, target_scheduled_times, encoder_hidden)\n",
        "\n",
        "        return residual_pred"
      ],
      "metadata": {
        "id": "S2898BwE6QUZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training function\n",
        "def train_model(model, train_loader, val_loader, epochs=50, learning_rate=0.001):\n",
        "    \"\"\"Train the encoder-decoder model\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} - Training'):\n",
        "            # Move data to device\n",
        "            trip_features = batch['trip_features'].to(device)\n",
        "\n",
        "            observed_times = batch['observed_times'].to(device)\n",
        "            observed_distances = batch['observed_distances'].to(device)\n",
        "            observed_scheduled_times = batch['observed_scheduled_times'].to(device)\n",
        "            observed_residual_times = batch['observed_residual_times'].to(device)\n",
        "\n",
        "            target_times = batch['target_times'].to(device)\n",
        "            target_distances = batch['target_distances'].to(device)\n",
        "            target_scheduled_times = batch['target_scheduled_times'].to(device)\n",
        "            target_residual_times = batch['target_residual_times'].to(device)\n",
        "\n",
        "            target_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            predicted_time_residuals = model(trip_features, observed_times, observed_distances,\n",
        "                                    observed_scheduled_times, observed_residual_times,\n",
        "                                             target_times, target_distances, target_scheduled_times)\n",
        "\n",
        "            # Compute loss (only for non-padded values)\n",
        "            loss = criterion(predicted_time_residuals, target_residual_times)\n",
        "            masked_loss = loss * target_mask\n",
        "            loss = masked_loss.sum() / (target_mask.sum() + 1e-8)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} - Validation'):\n",
        "                # Move data to device\n",
        "                trip_features = batch['trip_features'].to(device)\n",
        "\n",
        "                observed_times = batch['observed_times'].to(device)\n",
        "                observed_distances = batch['observed_distances'].to(device)\n",
        "                observed_scheduled_times = batch['observed_scheduled_times'].to(device)\n",
        "                observed_residual_times = batch['observed_residual_times'].to(device)\n",
        "\n",
        "                target_times = batch['target_times'].to(device)\n",
        "                target_distances = batch['target_distances'].to(device)\n",
        "                target_scheduled_times = batch['target_scheduled_times'].to(device)\n",
        "                target_residual_times = batch['target_residual_times'].to(device)\n",
        "\n",
        "                target_mask = batch['target_mask'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                predicted_time_residuals = model(trip_features, observed_times, observed_distances,\n",
        "                                        observed_scheduled_times, observed_residual_times,\n",
        "                                                 target_times, target_distances, target_scheduled_times)\n",
        "\n",
        "                # Compute loss (only for non-padded values)\n",
        "                loss = criterion(predicted_time_residuals, target_residual_times)\n",
        "                masked_loss = loss * target_mask\n",
        "                loss = masked_loss.sum() / (target_mask.sum() + 1e-8)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model, train_losses, val_losses"
      ],
      "metadata": {
        "id": "9JKr3Did6YnP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Function to make predictions\n",
        "def predict(model, test_loader, residual_time_scaler):\n",
        "    \"\"\"Generate predictions for test data\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    trip_info = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc='Generating predictions'):\n",
        "        # Move data to device\n",
        "            trip_features = batch['trip_features'].to(device)\n",
        "\n",
        "            observed_times = batch['observed_times'].to(device)\n",
        "            observed_distances = batch['observed_distances'].to(device)\n",
        "            observed_scheduled_times = batch['observed_scheduled_times'].to(device)\n",
        "            observed_residual_times = batch['observed_residual_times'].to(device)\n",
        "\n",
        "            target_times = batch['target_times'].to(device)\n",
        "            target_distances = batch['target_distances'].to(device)\n",
        "            target_scheduled_times = batch['target_scheduled_times'].to(device)\n",
        "            target_residual_times = batch['target_residual_times'].to(device)\n",
        "\n",
        "            target_mask = batch['target_mask'].to(device)\n",
        "\n",
        "            num_observed = batch['num_observed']\n",
        "            num_target = batch['num_target']\n",
        "\n",
        "            # Generate predictions\n",
        "            predicted_time_residuals = model(trip_features, observed_times, observed_distances,\n",
        "                        observed_scheduled_times, observed_residual_times,\n",
        "                                             target_times, target_distances, target_scheduled_times)\n",
        "\n",
        "            # Convert predictions back to original scale\n",
        "            predicted_time_residuals_np = predicted_time_residuals.cpu().numpy()\n",
        "            predicted_time_residuals_orig = residual_time_scaler.inverse_transform(\n",
        "                predicted_time_residuals_np.reshape(-1, 1)\n",
        "            ).reshape(predicted_time_residuals_np.shape)\n",
        "\n",
        "            # Convert targets back to original scale\n",
        "            target_residual_times_np = target_residual_times.cpu().numpy()\n",
        "            target_residual_times_orig = residual_time_scaler.inverse_transform(\n",
        "                target_residual_times_np.reshape(-1, 1)\n",
        "            ).reshape(target_residual_times_np.shape)\n",
        "\n",
        "            # Store results\n",
        "            for i in range(len(num_observed)):\n",
        "                n_obs = num_observed[i].item()\n",
        "                n_target = num_target[i].item()\n",
        "\n",
        "                # Extract valid predictions and targets\n",
        "                valid_pred = predicted_time_residuals_orig[i, :n_target]\n",
        "                valid_target = target_residual_times_orig[i, :n_target]\n",
        "\n",
        "                all_predictions.append(valid_pred)\n",
        "                all_targets.append(valid_target)\n",
        "                trip_info.append({\n",
        "                    'num_observed': n_obs,\n",
        "                    'num_target': n_target\n",
        "                })\n",
        "\n",
        "    return all_predictions, all_targets, trip_info"
      ],
      "metadata": {
        "id": "Vx6w0lK-6ewo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate predictions\n",
        "def evaluate_predictions(predictions, targets, trip_info):\n",
        "    \"\"\"Evaluate the quality of predictions\"\"\"\n",
        "    # Overall MAE and RMSE\n",
        "    all_pred = np.concatenate([p.flatten() for p in predictions])\n",
        "    all_target = np.concatenate([t.flatten() for t in targets])\n",
        "\n",
        "    mae = np.mean(np.abs(all_pred - all_target))\n",
        "    rmse = np.sqrt(np.mean((all_pred - all_target) ** 2))\n",
        "\n",
        "    print(f'Overall MAE: {mae:.2f} seconds')\n",
        "    print(f'Overall RMSE: {rmse:.2f} seconds')\n",
        "\n",
        "    # Plot a few examples\n",
        "    num_examples = min(20, len(predictions))\n",
        "    plt.figure(figsize=(15, 2*num_examples))\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        plt.subplot(num_examples, 1, i+1)\n",
        "\n",
        "        pred = predictions[i]\n",
        "        target = targets[i]\n",
        "        n_obs = trip_info[i]['num_observed']\n",
        "        n_target = trip_info[i]['num_target']\n",
        "\n",
        "        # Create x-axis based on relative position\n",
        "        x_obs = np.arange(n_obs)\n",
        "        x_target = np.arange(n_obs, n_obs + n_target)\n",
        "\n",
        "        # Plot the observed times (assuming we have them)\n",
        "        plt.scatter(x_obs, np.zeros(n_obs), color='gray', label='Observed stops')\n",
        "\n",
        "        # Plot predictions vs targets\n",
        "        plt.plot(x_target, pred, 'o-', label='Predicted')\n",
        "        plt.plot(x_target, target, 'x-', label='Actual')\n",
        "\n",
        "        plt.title(f'Trip Example {i+1}')\n",
        "        plt.xlabel('Stop Index')\n",
        "        plt.ylabel('Time (seconds)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate error by position\n",
        "    errors_by_position = []\n",
        "    max_targets = max(len(t) for t in targets)\n",
        "\n",
        "    for pos in range(max_targets):\n",
        "        pos_errors = []\n",
        "        for i in range(len(predictions)):\n",
        "            if pos < len(predictions[i]):\n",
        "                pos_errors.append(abs(predictions[i][pos] - targets[i][pos]))\n",
        "\n",
        "        if pos_errors:\n",
        "            errors_by_position.append(np.mean(pos_errors))\n",
        "\n",
        "    # Plot error by position\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(errors_by_position)), errors_by_position, 'o-')\n",
        "    plt.title('Mean Absolute Error by Stop Position')\n",
        "    plt.xlabel('Stops into the Future')\n",
        "    plt.ylabel('MAE (seconds)')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZuawZTQW6n4M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Main execution flow\n",
        "def main():\n",
        "    # Create dataset\n",
        "    print(\"Creating datasets...\")\n",
        "    dataset = BusTripDataset(df,\n",
        "                 trip_ids, route_encoder, day_encoder, time_scaler,\n",
        "                 distance_scaler, scheduled_time_scaler, residual_time_scaler)\n",
        "\n",
        "    # Split into train, validation, and test sets\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Calculate input dimensions\n",
        "    sample = next(iter(train_loader))\n",
        "    trip_feature_dim = sample['trip_features'].shape[1]\n",
        "\n",
        "    # Create and train the model\n",
        "    print(f\"Creating model with trip_feature_dim={trip_feature_dim}...\")\n",
        "    hidden_dim = 120\n",
        "    model = BusTimeEncoderDecoder(trip_feature_dim, hidden_dim)\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    model, train_losses, val_losses = train_model(\n",
        "        model, train_loader, val_loader, epochs=40)\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Generating predictions...\")\n",
        "    predictions, targets, trip_info = predict(model, test_loader, residual_time_scaler)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    print(\"Evaluating predictions...\")\n",
        "    evaluate_predictions(predictions, targets, trip_info)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'route_encoder': route_encoder,\n",
        "        'day_encoder': day_encoder,\n",
        "        'time_scaler': time_scaler,\n",
        "        'distance_scaler': distance_scaler,\n",
        "        'scheduled_time_scaler': scheduled_time_scaler,\n",
        "        'residual_time_scaler': residual_time_scaler,\n",
        "    }, 'bus_time_prediction_model.pth')\n",
        "\n",
        "    print(\"Model saved to 'bus_time_prediction_model.pth'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHBp9v5U6mhD",
        "outputId": "f448b1d5-324b-4b80-dc6b-673c67a8b888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets...\n",
            "Creating model with trip_feature_dim=30...\n",
            "Training model...\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50 - Training: 100%|| 92/92 [00:55<00:00,  1.65it/s]\n",
            "Epoch 1/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 1.1867, Val Loss: 1.4579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50 - Training: 100%|| 92/92 [00:56<00:00,  1.64it/s]\n",
            "Epoch 2/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50, Train Loss: 1.1764, Val Loss: 1.4537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50 - Training: 100%|| 92/92 [00:58<00:00,  1.57it/s]\n",
            "Epoch 3/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50, Train Loss: 1.1380, Val Loss: 1.4375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50 - Training: 100%|| 92/92 [00:55<00:00,  1.67it/s]\n",
            "Epoch 4/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50, Train Loss: 1.1197, Val Loss: 1.4050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50 - Training: 100%|| 92/92 [00:57<00:00,  1.61it/s]\n",
            "Epoch 5/50 - Validation: 100%|| 20/20 [00:10<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Train Loss: 1.1329, Val Loss: 1.1489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50 - Training: 100%|| 92/92 [00:55<00:00,  1.64it/s]\n",
            "Epoch 6/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50, Train Loss: 0.6602, Val Loss: 0.8377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50 - Training: 100%|| 92/92 [00:54<00:00,  1.67it/s]\n",
            "Epoch 7/50 - Validation: 100%|| 20/20 [00:12<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50, Train Loss: 0.4954, Val Loss: 0.6643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 8/50 - Validation: 100%|| 20/20 [00:10<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50, Train Loss: 0.4231, Val Loss: 0.6540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50 - Training: 100%|| 92/92 [00:55<00:00,  1.65it/s]\n",
            "Epoch 9/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50, Train Loss: 0.3676, Val Loss: 0.5883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 10/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Train Loss: 0.3570, Val Loss: 0.6247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50 - Training: 100%|| 92/92 [00:57<00:00,  1.61it/s]\n",
            "Epoch 11/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50, Train Loss: 0.3442, Val Loss: 0.5362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50 - Training: 100%|| 92/92 [00:55<00:00,  1.65it/s]\n",
            "Epoch 12/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50, Train Loss: 0.3104, Val Loss: 0.5189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50 - Training: 100%|| 92/92 [00:58<00:00,  1.58it/s]\n",
            "Epoch 13/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50, Train Loss: 0.3207, Val Loss: 0.5304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 14/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50, Train Loss: 0.3012, Val Loss: 0.4967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 15/50 - Validation: 100%|| 20/20 [00:12<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50, Train Loss: 0.2911, Val Loss: 0.4637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50 - Training: 100%|| 92/92 [00:55<00:00,  1.65it/s]\n",
            "Epoch 16/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50, Train Loss: 0.2885, Val Loss: 0.4616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50 - Training: 100%|| 92/92 [00:55<00:00,  1.65it/s]\n",
            "Epoch 17/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50, Train Loss: 0.2803, Val Loss: 0.4566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50 - Training: 100%|| 92/92 [00:58<00:00,  1.58it/s]\n",
            "Epoch 18/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50, Train Loss: 0.2460, Val Loss: 0.4066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 19/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50, Train Loss: 0.2265, Val Loss: 0.3926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50 - Training: 100%|| 92/92 [00:57<00:00,  1.61it/s]\n",
            "Epoch 20/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50, Train Loss: 0.2103, Val Loss: 0.3715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50 - Training: 100%|| 92/92 [00:57<00:00,  1.59it/s]\n",
            "Epoch 21/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50, Train Loss: 0.1920, Val Loss: 0.3625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50 - Training: 100%|| 92/92 [00:56<00:00,  1.61it/s]\n",
            "Epoch 22/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50, Train Loss: 0.1807, Val Loss: 0.3450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50 - Training: 100%|| 92/92 [00:57<00:00,  1.61it/s]\n",
            "Epoch 23/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50, Train Loss: 0.1980, Val Loss: 0.3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50 - Training: 100%|| 92/92 [00:57<00:00,  1.60it/s]\n",
            "Epoch 24/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50, Train Loss: 0.1857, Val Loss: 0.3245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50 - Training: 100%|| 92/92 [00:56<00:00,  1.62it/s]\n",
            "Epoch 25/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50, Train Loss: 0.1692, Val Loss: 0.3118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50 - Training: 100%|| 92/92 [00:58<00:00,  1.57it/s]\n",
            "Epoch 26/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50, Train Loss: 0.1628, Val Loss: 0.3245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 27/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50, Train Loss: 0.1605, Val Loss: 0.2971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50 - Training: 100%|| 92/92 [00:57<00:00,  1.61it/s]\n",
            "Epoch 28/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50, Train Loss: 0.1457, Val Loss: 0.2881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50 - Training: 100%|| 92/92 [00:57<00:00,  1.60it/s]\n",
            "Epoch 29/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50, Train Loss: 0.1440, Val Loss: 0.2821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50 - Training: 100%|| 92/92 [00:56<00:00,  1.62it/s]\n",
            "Epoch 30/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50, Train Loss: 0.1324, Val Loss: 0.2795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50 - Training: 100%|| 92/92 [00:58<00:00,  1.58it/s]\n",
            "Epoch 31/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50, Train Loss: 0.1254, Val Loss: 0.2635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50 - Training: 100%|| 92/92 [00:58<00:00,  1.58it/s]\n",
            "Epoch 32/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50, Train Loss: 0.1207, Val Loss: 0.2679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50 - Training: 100%|| 92/92 [00:56<00:00,  1.62it/s]\n",
            "Epoch 33/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50, Train Loss: 0.1268, Val Loss: 0.2610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50 - Training: 100%|| 92/92 [00:58<00:00,  1.59it/s]\n",
            "Epoch 34/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50, Train Loss: 0.1265, Val Loss: 0.2658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50 - Training: 100%|| 92/92 [00:57<00:00,  1.61it/s]\n",
            "Epoch 35/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50, Train Loss: 0.1206, Val Loss: 0.2656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50 - Training: 100%|| 92/92 [00:56<00:00,  1.61it/s]\n",
            "Epoch 36/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50, Train Loss: 0.1183, Val Loss: 0.3947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50 - Training: 100%|| 92/92 [00:56<00:00,  1.62it/s]\n",
            "Epoch 37/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50, Train Loss: 0.1595, Val Loss: 0.3663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50 - Training: 100%|| 92/92 [00:55<00:00,  1.65it/s]\n",
            "Epoch 38/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50, Train Loss: 0.1340, Val Loss: 0.2854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50 - Training: 100%|| 92/92 [00:57<00:00,  1.60it/s]\n",
            "Epoch 39/50 - Validation: 100%|| 20/20 [00:10<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50, Train Loss: 0.1203, Val Loss: 0.2816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50 - Training: 100%|| 92/92 [00:56<00:00,  1.62it/s]\n",
            "Epoch 40/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50, Train Loss: 0.1130, Val Loss: 0.4554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50 - Training: 100%|| 92/92 [00:55<00:00,  1.66it/s]\n",
            "Epoch 41/50 - Validation: 100%|| 20/20 [00:12<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50, Train Loss: 0.1370, Val Loss: 0.3646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 42/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50, Train Loss: 0.1325, Val Loss: 0.3341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50 - Training: 100%|| 92/92 [00:55<00:00,  1.66it/s]\n",
            "Epoch 43/50 - Validation: 100%|| 20/20 [00:11<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50, Train Loss: 0.1116, Val Loss: 0.3366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50 - Training: 100%|| 92/92 [00:56<00:00,  1.63it/s]\n",
            "Epoch 44/50 - Validation:  75%|  | 15/20 [00:08<00:02,  1.88it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Function for inference on new data\n",
        "def predict_bus_times(model_path, new_trip_data, observed_stops):\n",
        "    \"\"\"\n",
        "    Predict bus times for remaining stops in a trip\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model\n",
        "        new_trip_data: DataFrame containing trip data\n",
        "        observed_stops: List of stop IDs that have been observed\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with predicted arrival times for remaining stops\n",
        "    \"\"\"\n",
        "    # Load the model and preprocessing objects\n",
        "    checkpoint = torch.load(model_path)\n",
        "\n",
        "    # Create model\n",
        "    trip_feature_dim = len(checkpoint['route_encoder'].categories_[0]) + len(checkpoint['day_encoder'].categories_[0])  # routes + days\n",
        "    hidden_dim = 100\n",
        "    model = BusTimeEncoderDecoder(trip_feature_dim, hidden_dim)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Get the preprocessing objects\n",
        "    route_encoder = checkpoint['route_encoder']\n",
        "    day_encoder = checkpoint['day_encoder']\n",
        "    time_scaler = checkpoint['time_scaler']\n",
        "    distance_scaler = checkpoint['distance_scaler']\n",
        "    scheduled_time_scaler = checkpoint['scheduled_time_scaler']\n",
        "    residual_time_scaler = checkpoint['residual_time_scaler']\n",
        "\n",
        "    # Extract trip details\n",
        "    trip_id = new_trip_data['id'].iloc[0]\n",
        "    route_name = new_trip_data['route_name'].iloc[0]\n",
        "    day = new_trip_data['day'].iloc[0]\n",
        "\n",
        "    # Filter to observed stops\n",
        "    observed_df = new_trip_data[~new_trip_data['residual_stop_time'].isna()]\n",
        "    remaining_df = new_trip_data[new_trip_data['residual_stop_time'].isna()]\n",
        "\n",
        "    if observed_df.empty or remaining_df.empty:\n",
        "        print(\"Error: No observed stops or no remaining stops to predict\")\n",
        "        return None\n",
        "\n",
        "    # Sort by distance\n",
        "    # observed_df = observed_df.sort_values('stop_distance')\n",
        "    # remaining_df = remaining_df.sort_values('stop_distance')\n",
        "\n",
        "    # Prepare features\n",
        "    # One-hot encode route\n",
        "    route_name_df = pd.DataFrame([[route_name]], columns=['route_name'])\n",
        "    route_encoded = route_encoder.transform(route_name_df)[0]\n",
        "\n",
        "    # One-hot encode day\n",
        "    day_df = pd.DataFrame([[day]], columns=['day'])\n",
        "    day_encoded = day_encoder.transform(day_df)[0]\n",
        "\n",
        "    # Trip features\n",
        "    trip_features = np.concatenate([\n",
        "        route_encoded,\n",
        "        day_encoded\n",
        "    ])\n",
        "    trip_features = torch.tensor(trip_features, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Observed stops\n",
        "    observed_times = time_scaler.transform(observed_df[['time']])\n",
        "    observed_distances = distance_scaler.transform(observed_df[['stop_distance']])\n",
        "    observed_scheduled_times = scheduled_time_scaler.transform(observed_df[['scheduled_stop_time']])\n",
        "    observed_residual_times = residual_time_scaler.transform(observed_df[['residual_stop_time']])\n",
        "\n",
        "    observed_times = torch.tensor(observed_times, dtype=torch.float32).unsqueeze(0)\n",
        "    observed_distances = torch.tensor(observed_distances, dtype=torch.float32).unsqueeze(0)\n",
        "    observed_scheduled_times = torch.tensor(observed_scheduled_times, dtype=torch.float32).unsqueeze(0)\n",
        "    observed_residual_times = torch.tensor(observed_residual_times, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    # Target stops (for which we want predictions)\n",
        "    target_times = time_scaler.transform(remaining_df[['time']])\n",
        "    target_distances = distance_scaler.transform(remaining_df[['stop_distance']])\n",
        "    target_scheduled_times = scheduled_time_scaler.transform(remaining_df[['scheduled_stop_time']])\n",
        "\n",
        "    target_times = torch.tensor(target_times, dtype=torch.float32).unsqueeze(0)\n",
        "    target_distances = torch.tensor(target_distances, dtype=torch.float32).unsqueeze(0)\n",
        "    target_scheduled_times = torch.tensor(target_scheduled_times, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        predicted_time_residuals = model(trip_features,\n",
        "                                         observed_times, observed_distances, observed_scheduled_times, observed_residual_times,\n",
        "                                         target_times, target_distances, target_scheduled_times)\n",
        "\n",
        "    # Convert predictions back to original scale\n",
        "    predicted_time_residuals_np = predicted_time_residuals.numpy().reshape(-1, 1)\n",
        "    predicted_time_residuals_orig = residual_time_scaler.inverse_transform(predicted_time_residuals_np).flatten()\n",
        "\n",
        "    # Add predictions to the dataframe\n",
        "    remaining_df['predicted_time'] = predicted_time_residuals_orig\n",
        "\n",
        "    # Combine all stops for display\n",
        "    all_stops_df = pd.concat([\n",
        "        observed_df,\n",
        "        remaining_df\n",
        "    ]).sort_values('stop_distance')\n",
        "\n",
        "    return all_stops_df"
      ],
      "metadata": {
        "id": "rBmKBqDJ6s_-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BusTimesInference():\n",
        "    \"\"\"\n",
        "    Class holding the bus time prediction model\n",
        "    Offering prediction method\n",
        "    \"\"\"\n",
        "\n",
        "    HIDDEN_DIM = 120\n",
        "    OVERLAP = 3\n",
        "\n",
        "    def __init__(self, model_filename):\n",
        "        # Load the model and preprocessing objects\n",
        "        model_path = get_root() / \"models\" / model_filename\n",
        "        checkpoint = torch.load(model_path, weights_only=False)\n",
        "\n",
        "        # Create model\n",
        "        self.trip_feature_dim = len(checkpoint['route_encoder'].categories_[0]) + len(checkpoint['day_encoder'].categories_[0])  # routes + days\n",
        "        self.model = BusTimeEncoderDecoder(self.trip_feature_dim, self.HIDDEN_DIM)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        # Get the preprocessing objects\n",
        "        self.route_encoder = checkpoint['route_encoder']\n",
        "        self.day_encoder = checkpoint['day_encoder']\n",
        "        self.time_scaler = checkpoint['time_scaler']\n",
        "        self.distance_scaler = checkpoint['distance_scaler']\n",
        "        self.scheduled_time_scaler = checkpoint['scheduled_time_scaler']\n",
        "        self.residual_time_scaler = checkpoint['residual_time_scaler']\n",
        "\n",
        "    def get_trip_features(self, new_trip_data):\n",
        "        \"\"\"Prepares the trip_features\"\"\"\n",
        "        # Extract trip details\n",
        "        # trip_id = new_trip_data['id'].iloc[0]\n",
        "        route_name = new_trip_data['route_name'].iloc[0]\n",
        "        day = new_trip_data['day'].iloc[0]\n",
        "\n",
        "        # Prepare features\n",
        "        # One-hot encode route\n",
        "        route_name_df = pd.DataFrame([[route_name]], columns=['route_name'])\n",
        "        route_encoded = self.route_encoder.transform(route_name_df)[0]\n",
        "\n",
        "        # One-hot encode day\n",
        "        day_df = pd.DataFrame([[day]], columns=['day'])\n",
        "        day_encoded = self.day_encoder.transform(day_df)[0]\n",
        "\n",
        "        # Trip features\n",
        "        trip_features = np.concatenate([\n",
        "            route_encoded,\n",
        "            day_encoded\n",
        "        ])\n",
        "        trip_features = torch.tensor(trip_features, dtype=torch.float32).unsqueeze(0)\n",
        "        return trip_features\n",
        "\n",
        "    def get_observed_data(self, observed_df):\n",
        "        \"\"\"Extract the stop features for observed\"\"\"\n",
        "        observed_times = self.time_scaler.transform(\n",
        "                observed_df[['time']])\n",
        "        observed_distances = self.distance_scaler.transform(\n",
        "                observed_df[['distance_to_stop']])\n",
        "        observed_scheduled_times = self.scheduled_time_scaler.transform(\n",
        "                observed_df[['time_to_stop']])\n",
        "        observed_residual_times = self.residual_time_scaler.transform(\n",
        "                observed_df[['residual_stop_time']])\n",
        "\n",
        "        observed_times = torch.tensor(\n",
        "                observed_times, dtype=torch.float32).transpose(0, 1)\n",
        "        observed_distances = torch.tensor(\n",
        "                observed_distances, dtype=torch.float32).transpose(0, 1)\n",
        "        observed_scheduled_times = torch.tensor(\n",
        "                observed_scheduled_times, dtype=torch.float32).transpose(0, 1)\n",
        "        observed_residual_times = torch.tensor(\n",
        "                observed_residual_times, dtype=torch.float32).transpose(0, 1)\n",
        "\n",
        "        return observed_times, observed_distances, observed_scheduled_times, observed_residual_times\n",
        "\n",
        "    def get_target_data(self, remaining_df):\n",
        "        \"\"\"Extract the stop features for target\"\"\"\n",
        "        target_times = self.time_scaler.transform(\n",
        "                remaining_df[['time']])\n",
        "        target_distances = self.distance_scaler.transform(\n",
        "                remaining_df[['distance_to_stop']])\n",
        "        target_scheduled_times = self.scheduled_time_scaler.transform(\n",
        "                remaining_df[['time_to_stop']])\n",
        "\n",
        "        target_times = torch.tensor(\n",
        "                target_times, dtype=torch.float32).transpose(0, 1)\n",
        "        target_distances = torch.tensor(\n",
        "                target_distances, dtype=torch.float32).transpose(0, 1)\n",
        "        target_scheduled_times = torch.tensor(\n",
        "                target_scheduled_times, dtype=torch.float32).transpose(0, 1)\n",
        "\n",
        "        return target_times, target_distances, target_scheduled_times\n",
        "\n",
        "    def split_dataframes(self, new_trip_data):\n",
        "        \"\"\"Splits the dataframe to observed and target\"\"\"\n",
        "        observed_df = new_trip_data[~new_trip_data['residual_stop_time'].isna()]\n",
        "        remaining_df = new_trip_data[new_trip_data['residual_stop_time'].isna()]\n",
        "        if observed_df.empty or remaining_df.empty:\n",
        "            return [None, None]\n",
        "        return observed_df, remaining_df\n",
        "\n",
        "    def add_overlap(self, observed_df, remaining_df):\n",
        "        \"\"\"Add the overlapping stops from observed to remaining\"\"\"\n",
        "        overlap_rows = observed_df.tail(self.OVERLAP)\n",
        "        remaining_df = pd.concat([overlap_rows, remaining_df], ignore_index=True)\n",
        "        return remaining_df\n",
        "\n",
        "    def remove_overlap(self, predicted_time_residuals):\n",
        "        \"\"\"Remove the overlapped predicted stops\"\"\"\n",
        "        return predicted_time_residuals[self.OVERLAP:]\n",
        "\n",
        "    def rescale_predictions(self, predicted_time_residuals):\n",
        "        \"\"\"Convert predicted times back to scale\"\"\"\n",
        "        # Convert predictions back to original scale\n",
        "        predicted_time_residuals_np = predicted_time_residuals.numpy().reshape(-1, 1)\n",
        "        predicted_time_residuals_orig = self.residual_time_scaler.inverse_transform(predicted_time_residuals_np).flatten()\n",
        "        return predicted_time_residuals_orig\n",
        "\n",
        "    def predict_trip(self, trip):\n",
        "        \"\"\"\n",
        "        Predict the bus trip for the next target stops\n",
        "        \"\"\"\n",
        "        if not trip.inference_eligible():\n",
        "            raise ValueError(\"Trip did not pass enough stops (3)\")\n",
        "        observed_df, remaining_df = trip.observed_df, trip.target_df\n",
        "        if observed_df is None or remaining_df is None:\n",
        "            raise ValueError(\"Failed in resolving the dataframes\")\n",
        "        remaining_df = self.add_overlap(observed_df, remaining_df)\n",
        "        trip_features = self.get_trip_features(pd.concat([observed_df, remaining_df]))\n",
        "        observed_times, observed_distances, observed_scheduled_times, observed_residual_times = self.get_observed_data(observed_df)\n",
        "        target_times, target_distances, target_scheduled_times = self.get_target_data(remaining_df)\n",
        "        with torch.no_grad():\n",
        "            predicted_time_residuals = self.model(trip_features,\n",
        "                                             observed_times, observed_distances, observed_scheduled_times, observed_residual_times,\n",
        "                                             target_times, target_distances, target_scheduled_times)\n",
        "        predicted_time_residuals = self.rescale_predictions(predicted_time_residuals)\n",
        "        # predicted_time_residuals = predicted_time_residuals# .squeeze(0)\n",
        "        predicted_time_residuals = self.remove_overlap(predicted_time_residuals)\n",
        "        trip.add_predictions(predicted_time_residuals)\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "xnB7HAs3_yKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to use the inference function:\n",
        "# new_trip_data = pd.read_csv('new_trip.csv')\n",
        "# observed_stops = ['8380B2407401', '8380B2407501', '8380B2407601']\n",
        "# predictions = predict_bus_times('bus_time_prediction_model.pth', new_trip_data, observed_stops)\n",
        "# print(predictions)"
      ],
      "metadata": {
        "id": "fWypPlbj60uY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}